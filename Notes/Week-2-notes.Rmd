---
title: "Week 2 Notes"
author: "Zack Wixom"
output: github_document
---

## Chapter 1: _Golem of Prague_

Computers are like _Golems_. They do what we tell them but can end up destroying things because it doesn't know our intent. 

## Installing STAN

```{r install stan}
# # we recommend running this is a fresh R session or restarting your current session
# install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```

Now that Stan is installed we can call `cmdstanr` package to use Stan, as well as `posterior` and `bayesplot`

```{r}
# Packages
library(tidyverse)
library(cmdstanr)
library(posterior)
library(bayesplot)
color_scheme_set("brightblue")

# Use check_cmdstan_toolchain() to see if cmdstanr is working

# then install cmdstanr with
install_cmdstan(cores = 2)
```

## Statistical Rethinking package

Now to install the package that goes along with the book we are using

```{r}
install.packages(c("coda", "mvtnorm", "devtools", "loo"))
devtools::install_github("rmcelreath/rethinking")
```


# What is Bayesian Data Analysis

Using probability as a language to describe uncertainty

- Extends ordinary binary logic (True/False) to continuous plausibilities

Difficult to do so we use MCMC (Markov chain Monte Carlo)

Used to be controversial in the stats world

Into one statement:

__Count all the ways data can happen, according to your assumptions. The assumptions with more ways that are consistent with data are more plausible.__

## Multi-level Models

__Models within models__

- Models with multiple levels of uncertainty
- Replace parameters with models

__Common uses__

- Repeat & Imbalance sampling
- Study variation
- Avoid averaging
- Phlyogenetics, factor and path analysis, networks and spatial models

Natural Bayesian strategy


## Notes from Class on Golem of Prague

### Epistemology

We learn from _data_. and update what we believe about the world around us. It isn't a binary condition. We have varying levels of certainty and uncertainty.

We are offered pre-constructed golems known as "test". What we need is a theory of golem engineering, a set of principles of designing, building, and refinidng special-purpose statistical procedures

From the book:

- Models are needed to learn about something that is unobserved _and most likely unobservable_.
- If we contruct models explicitly we can compare models to evaluate evidence for competiting theories. Isn't of one way that is specific way you use multiple models to answer a question.
- Bayesian inference is a "single method for building"

### Bayesian Data Analysis

is no more than counting the numbers of ways the data could happen according to our assumptions

Bayesian Inference is what happens when you take probability theory seriously

### The Bayesian Triptych

__Three steps__

- Prior 
  + the assumptions
- Fit Model
  + observe data
- Posterior
  + update assumptions
  

### Model Comparison and Prediction

Inferential Models are used to understand the underlying process that resulted in the data we observed.
- We call predictive models predictive because that's all they do - no inference.

Scientific terms...
- Think of a model as a theory. a competing model is therefore a competing theory. Comparing theories/models and learning as we develop a theory

Business terms...
- think of a model as our paradigm of the underlying choice. use it to determine which choice is preferred. 

### Multi-level/Hierarchical Models

The regression models we are used to are single-level flat or aggregate. 

Hierarchical models should be default because they recognize that heterogeneity exists (different people make different choices)


We need to get information of group differences for our project
- segment a person is in
- treat each individual as a group

Need to have some info in the data that allows you to split the data into groups


### Graphical Causal models

Successful prediction does not require correct casual identification. 

Casual identification fits within realm of inference. Predictive models are constructed to ignore causality and inference in service of prediction.

### Ending comments

Truth comes from consensus, there is always a degree of uncertainty. The more experiences, or data the smaller the degree but science doesn't make truth. Same goes for faith. 



## Chapter 2: _Garden of Forking_ 

### Building a Model

How to use probability to do typical statistical modeling?

1. __Design__ the model, _data story_ or _data generation_

2. __Condition__ on the data, _update_

3. __Evaluate__ the model, _critique_


### Throwing the Globe

Nine tosses of the globe:

__W L W W W L W L W__

Goal : Attempt to estimate the percent of water there is on the earth. 

How do we use this information?

### Design

Data story motivates the model
- how does the data arise?

Creates a model that generates data.

Descriptive vs. Casual

For W L W W W L W L W:

- Some true proportion of water, _p_
- toss the globe, probably _p_ of observing W, 1-p of L
- Each toss therefore independent of other tosses

Translate data story into probability statements

### Condition

Bayesian updating defines optimal learning in small world, converts prior (old assumption) into posterior (new assumption)
- Give your golem an information state, before the data:
  + Here , and initial confidence in each possible value of _p_ is between zero and one

- Condition on data to update information: 
  + New confidence in each value
  + Slicing the non plausible values
  
Each posterior becomes new prior as you update.

Each additional information contributes less marginal change in the model at some point.

Bad initial plausibility lead to misleading inference. small samples doesn't discredit. 

Order is irrelevant. since each observation is independent.

Sample size automatically embodied in posterior.

### Evaluate

Bayesian Inference: __Logical Answer to a question in the form of a model__

"How plausible is each proporation of water, give these data?"

Golem must be supervised.

- biases can happen

- use posterior checks

### Construction Perspective

__Build joint model__:

1. List variables
2. Define generate relations
3. ???
4. Profit

__Input__: Joint Prior

__Deduce__: Joint Posterior

### Globe example

1. W, N, _p_

#### W

Relative number of ways to see W given N and _p_?

WLW

px(1-p)xp = p^2(1-p)^1

Sequence doesn't matter so you need to pay attention to the different orders of the WLW: LWW, WWL, WLW

So let's code this math. Not sure what this is telling me though, is this the likelihood of W occurring or of L? or something different?

`dbinom()` is distribution binomial

```{r}

# Find likelihood? W = 6, N = 9, p = 0.5 
dbinom(6, size = 9, prob = 0.5)

```

### Prior Probability

### the Joint Model

W ~ Binomial (N,p)

p ~ Uniform (0,1)

### Posterior Probability

Bayesian estimate is always posterior distribution over parameters (not a point).

Take it or leave it you don't have a choice.

Posterior = ( (pro observed variables) x (prior) )/ Normalizing constant, _average probability of data_

### Computing the posterior 

1. Analytics approach (impossible)
2. Grid approximation (very intensive)
3. Quadratic or Laplace approximation (limited)
4. Markov Chain Monte Carlo (intensive) 

### Grid approximation
- Posterior probably is standardized product of 1) probability of the data 2) prior probability

Standardized = add up all the products and divide each by the sum

Computation:

```{r}
# Define the Grid (all values you will consider)
p_grid <- seq(0, 1, length.out = 20)

# Define Prior (unifrom distrubtion of 1's)
prior <- rep(1, 20)

# Probability of the data
likelihood <- dbinom(6, size = 9, prob = p_grid)

# Compute Product of likelhood and prior
unstd_posterior <- likelihood * prior

# standardized the posterior so it sums to 1
posterior <- unstd_posterior / sum(unstd_posterior)

plot(p_grid, posterior, type = "b", xlab = "probability of water", ylab = "posterior probability")

mtext("20 points")

## To update the prior Having some issues with the code
prior <- ifelse( p_grid < 0.5 , < , 1) 

prior <- exp( -5*abs( p_grid - 0.5 ) )"


```

### Random Numbers

We use computers to make random numbers and then make inferences from them.

Sampling from the posterior

- Incredible useful sample randomly from posterior
  + Visualize uncertainty
  + Compute confidence intervals
  + Simulate observations
  
- MCMC produces only samples

- Above all, _easier to think with samples_

- Transform a hard calculus problem into an easy data summary problem. 

__Recipe__

1. Compute or approximate posterior

2. Sample with replacement from posterior

3. Compute stuff from samples


```{r}
# not sure if p is prob_p or not
samples <- sample(prior, prob = posterior, size = 1e4, replace = TRUE)

```

### Compute stuff

Summary tasks
- how much posterior prob is above or below zero
- which pararmeter value contains...

Two kinds of intervals

- __Percentile Intervals__ (PI)
  + equal area in each tail
  + Good at communicating the shape of distribution if it isn't too asymmetrical.

- __Highest posterior density intervals__(HPDI)
  + narrowest interval containing mass
  + always includes highest point
  
```{r}
# in rethinking package couldn't get it to work though

library(rethinking)

PI(samples, prob = 0.5)

HPDI(samples, prob = 0.5)


```

Point estimates not the point

Don't usually want point estimates
- entire posterior contains more information
- best point depends on purpose
- mean nearly always more sensible than mode

Intervals

_Confidence Intervals_
- A non-Bayesian term that doesn't even mean what it says

_Credible Interval_
- The values are not "credible" unless you trust the model & data

_Compatibility interval_
- Interval contains values compatible with model and data as provided
- Small World interval


### Predictive Checks
- Posterior Probability never enough
- Even the best model might make terrible predictions
- Check model assumptions
- Predictive Checks: Can use samples from posterior to simulate observations
  + NB: Assumption about sampling is assumption
  
```{r}
nw <- rbinom(1e4, size = 9, prob = samples)

```
  
## Notes from Class on Garden of Forking

### Building a Model

Questions: what is the proportion of earth's surface

- Simple goals is good

Data: Sequence of observations

Assumptions help constitute our model, then try to condense the "large world" into the "small world".

### Sampling the Imaginary

Bayes is unique because it's distinct view of probability

Frequencies can be confusing - we don't encounter them naturally

Sampling helps us avoid messy math (integrals)

Many of our motors produce samples Like MCMC




  














