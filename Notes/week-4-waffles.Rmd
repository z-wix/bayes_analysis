---
title: "Spurious Waffles"
author: "Zack Wixom"
output: github_document
---

```{r opts, echo = FALSE}
# puts any figures made into this folder
knitr::opts_chunk$set(
  fig.path = "../Figures/"
)
```

## Waffles and Divorce

![waffles and divorce](/Users/zwixom/School/Marketing/Quant Analytics/Repo/zack-wixom/Figures/wafflesanddivorce.png)

## Correlations

Commonplace in nature.

*Caution* can be misleading.

Correlation does not imply Causation and vice versa.

Causation implies *conditional* correlation

## Multiple Regression Models

*Using more than one predictor variable to model an outcome*.

**The Good**

-   Reveal spurious correlations

-   Uncover masked associations

**The Bad**

-   Causes spurious correlations and confounds

-   Hides real associations

**The DAGs**

-   Tool for Casual Models with causal implications

-   Allows you to describe qualitiative casual relationships among variables

## Reasons for Multiple Regression

1. Statistical controls for *confounds*.

2. Multiple and complex causation.

3. Interactions.


**Confounds**

Misleads us about a causal influence, such as the Waffle House correlation with Divorce. Makes some variables with no real importance look important. 

Sometimes confounds hide important effects as well.

**Complex Causation**

must measure causes simultaneously becuase there can be multiple causes happening at the same time and cascading in complex ways.

**Interactions**

Importance of one variable can depend on another. Interactions happen often so must consider others to get correct effective inference.


## Spurious Association

We are looking at different variables that are associated with divorce. Marriage Rate (positive association) and the Median Age at Marriage (negative association).

```{r 5.1}
# Load packages
library(tidyverse)
library(rethinking)

# Load Data
data(WaffleDivorce)
d <- WaffleDivorce

# Standardize Variables
d$D <- standardize(d$Divorce)           # Divorce Rate
d$M <- standardize(d$Marriage)          # Marriage Rate
d$A <- standardize(d$MedianAgeMarriage) # Median Age at marriage

```

D = standardized divorce rate for the state

A = standardized median age at marriage for the state

M = standardized marriage rate for the state

Since we standardized the variables we expect the mean of a to be around zero. 

In order to know if there is a strong relationship or not, you need to know the standard deviation of age at marriage. 

```{r}

sd(d$MedianAgeMarriage)

```

So when there is a prior slope of 1 then a change of 1.2 years in median age is associated. 

??

## Median Age and Divorce

```{r median age and divorce pt1}
# Fitting model

m5.1 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA * A,        # Median Age as Predictor
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)


```

What is dexp() doing here?

### Simulating from the priors

Using `extract.prior` and `link` then plot lines over the range of 2 standard deviatiosn for both the outcome and predictor.

```{r median age and divorce pt2}
set.seed(10)

prior <- extract.prior(m5.1)

mu <- link(m5.1, post = prior, data = list(A = c(-2, 2)))

plot(NULL, xlim = c(-2, 2), ylim = c(-2,2))
for (i in 1:50) {
  lines(c(-2,2), mu[i,], col = col.alpha("black", 0.4))
}

```

### Posterior Predictions

```{r median age and divorce pt3}
# Compute percentile interval of mean
A_seq <- seq(-3, 3.2, length.out = 30)
mu <- link(m5.1, data = list(A = A_seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# Plot 
plot(D ~ A, data = d, col = rangi2)
lines(A_seq, mu.mean, lwd = 2)
shade(mu.PI, A_seq)

```

There is a negative relationship for Median Age posteriors. `precis` confirms this as well.

```{r}
precis(m5.1)
```



## Marriage rate and Divorce

```{r marriage and divorce}
## Model
m5.2 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM * M,        # Marriage Rate as Predictor
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

## Simulating Priors

set.seed(10)

prior <- extract.prior(m5.2)

mu <- link(m5.2, post = prior, data = list(M = c(-2, 2)))

plot(NULL, xlim = c(-2, 2), ylim = c(-2,2))
for (i in 1:50) {
  lines(c(-2,2), mu[i,], col = col.alpha("black", 0.4))
}

## Posterior Predictions

# Compute percentile interval of mean
M_seq <- seq(-3, 3.2, length.out = 30)
mu <- link(m5.2, data = list(M = M_seq))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI)

# Plot 
plot(D ~ M, data = d, col = rangi2)
lines(M_seq, mu.mean, lwd = 2)
shade(mu.PI, M_seq)


```

There is a positive relationship from the posteriors of Marriage Rate, but not as strong as the Median Age. 

```{r}

precis(m5.2)

```

## Directed Acyclic Graph 

![dagimage](/Users/zwixom/School/Marketing/Quant Analytics/Repo/zack-wixom/Figures/dagimage.png)

DAGs will tell you the consequences of interveneing to change a variable if the DAG is correct. 

What is happening?

1. A *directly influences* D

2. M *directly influences* D

3. A *directly influences* M

We need more than one model to infer the strength of each relationship.

**Mediation** relationships are where a variable has no direct effect on outcome but is still associated with the outcome through an indirect relationship with another variable. 

```{r drawing dags}
library(dagitty)
dag5.1 <- dagitty("dag{A -> D; A -> M; M -> D}")
coordinates(dag5.1) <- list(x = c(A = 0, D = 1, M = 2), y = c(A = 0, D = 1, M = 0))
drawdag(dag5.1)
```

## Testable Implications

How we use data to compare multiple plausible causal model.

Any DAG may imply that some variables are independent of others under certain conditions.

**Conditional Independencies**

1. Which variables are associated or not with another in the data.

2. Which variables become *dis-associated* when we condition on some other set of variables.

![twodagimage](/Users/zwixom/School/Marketing/Quant Analytics/Repo/zack-wixom/Figures/twodags.png)

### Right DAG

Shows the implied conditional independencies

```{r dag2 condition}
DMA_dag2 <- dagitty('dag{D <- A -> M}')
impliedConditionalIndependencies(DMA_dag2)

```

### Left DAG

No conditional independecies

```{r dag1 condition}
DMA_dag1 <- dagitty('dag{D <- A -> M -> D}')
impliedConditionalIndependencies(DMA_dag1)
```

In order to test the implied conditional independencies for Right DAG we need to condition on Variable A. Then we can see if D is independent of M by using Multiple Regression.

The question we want to answer:

*After I already know Variable A, what addiitonal value is there in also knowing Variable B?*


## Multiple Regression Notation

1. Nominate the predictor variables.

2. Make parameter that will measure its conditional association.

3. Multiply the parameter by the variable and add that term to model.


![formulaimage](/Users/zwixom/School/Marketing/Quant Analytics/Repo/zack-wixom/Figures/formulapic.png)

![formul2aimage](/Users/zwixom/School/Marketing/Quant Analytics/Repo/zack-wixom/Figures/formulcode.png)

### Approximating the posterior

Code to approximate the posterior distribution

```{r approx post}

m5.3 <- quap(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

precis(m5.3)

```

Let's plot all of the models sides by side

```{r side plot}

plot(coeftab(m5.1, m5.2, m5.3), par = c("bA", "bM"))

```

bA or the posterior mean for median age at marriage is almost unchanged.

### Simulating the Divorce

```{r}

N <- 50                 # Number of simulated States
age <- rnorm(N)         # Sim A
mar <- rnorm(N, -age)   # Sim A -> M
div <- rnorm(N, age)    # Sim A -> D

```

## Ploting Multivariate Posteriors

let's visualize the model's inferences so we can understand. With only one predictor variable we can use a scatter plot.

**Predictor Residual Plot**

Show outcome against residual predictor values. Good for understanding statistical models.

Since we standardized the variables we expect the mean of a to be around zero. 

```{r}
m5.4 <- quap(
  alist(
    M ~ dnorm(mu, sigma),
    mu <- a + bAM * A,
    a ~ dnorm(0, 0.2),
    bAM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

# Compute Residuals

mu <- link(m5.4)
mu_mean <- apply(mu, 2, mean)
mu_resid <- d$M - mu_mean

```
When a residual is positive that means that the observed rate was in excess of what the model expects, given the median age at marriage in that state.

States with postitive residuals have high marriage rates for their median age and those with negative residuals have low marriage rates for their age.

*Note*: Residuals are parameters not data. Can't use them in another model.

**Posterior Prediction Plot**

Model based predictions against raw data. They are tools for checking fit and assessing predictions. Not causal tools.

Two uses

- Did model correctly approximated the posterior distribution?

- Does the model fail?

```{r}
# Call link without specifying new data so it uses the original data
mu <- link(m5.3)

# Summarize samples across cases
mu_mean <- apply(mu, 2, mean)
mu_PI <- apply(mu, 2, PI)

# Simulate observation again no new data
D_sim <- sim(m5.3, n = 1e4)
D_PI <- apply(D_sim, 2, PI)

# Plot predictions
plot(mu_mean ~ d$D, col = rangi2, ylim = range(mu_PI),
     xlab = "Observed divorce", ylab = "Predicted divorce")

abline(a = 0, b = 1, lty = 2)

for (i in 1:nrow(d)) {
  lines(rep(d$D[i], 2), mu_PI[,i], col = rangi2)
}

```


Not sure how this works

```{r}
# identify(x = d$D, y = mu_mean, labels = d$Loc)
```

*Shoutout to Mormons*

## Simulating Spurious association

```{r}

N <- 100                            # number of cases
x_real <- rnorm(N)                  # Gaussian with mean 0, sd 1
x_spur <- rnorm(N, x_real)          # Gaussian with mean = x_real
y <- rnorm(N, x_real)               # Gaussian with mean = x_real
d <- data.frame(y, x_real, x_spur)  # bind all together in data frame

```

x_real is influencing both y and x_spur. Both x_spur and x_real are correlated with y.

**Counterfactual Plot**

Show the implied predictions for imaginary experiments. Allow you to explore the causal implications of manipulating one or more variables.

They can be produced for any values of the predictor variables you like.

How to generate plots that take causal structure into account:

1. Pick variable to manipulate, the intervention variable.

2. Define the range of values to set the intervention variable to.

3. For each value of the intervention variable, and for each sample in posterior, use the causal model to simulate the values of other variables, including the outcome. 

We are going to simulatre form the DAG from before. 

```{r}
data(WaffleDivorce)
d <- list()
d$A <- standardize(WaffleDivorce$MedianAgeMarriage)
d$D <- standardize(WaffleDivorce$Divorce)
d$M <- standardize(WaffleDivorce$Marriage)
m5.3_A <- quap(
  alist(
    ## A -> D <- M
    D ~ dnorm(mu, sigma),
    mu <- a + bM*M + bA*A,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1),
    ## A -> M
    M ~ dnorm(mu_M, sigma_M),
    mu_M <- aM + bAM*A,
    aM ~ dnorm(0, 0.2),
    bAM ~ dnorm(0, 0.5),
    sigma_M ~ dexp(1)
  ), data = d
)

precis(m5.3_A)
```

The goal is to simulate so we get a range of values for A

```{r}

A_seq <- seq(from = -2, to = 2, length.out = 30)

# Prep Data
sim_dat <- data.frame(A = A_seq)

# Simulate M and then D, using A_seq
s <- sim(m5.3_A, data = sim_dat, vars = c("M", "D"))


# Plot
plot(sim_dat$A, colMeans(s$D), ylim = c(-2, 2), type = "l",
     xlab = "manipulated A", ylab = "counterfactual D")
shade(apply(s$D, 2, PI), sim_dat$A)
mtext("Total counterfactual effect of A on D")

```

You can also get numerical summaries

```{r}
# New data frame, standardized to mean 26.1 and sd 1.24
sim2_dat <- data.frame(A = (c(20, 30)-26.1)/1.24)
s2 <- sim(m5.3_A, data = sim2_dat, vars = c("M", "D"))
mean(s2$D[,2] - s2$D[,1])


```

This is a huge effect of four and one half stdeviations so probably impossible.

The trick with simulating counterfactuals is to realize that when we manipulate some variable X, we break the casual influence of other variables on X. 

```{r}

sim_dat <- data.frame(M = seq(from = -2, to = 2, length.out = 30), A = 0)
s <- sim(m5.3_A, data = sim_dat, vars = "D")

plot(sim_dat$M, colMeans(s), ylim = c(-2,2), type = "l",
     xlab = "manipulated M", ylab = "counterfactual D")
shade(apply(s,2, PI), sim_dat$M)
mtext("Total counterfactual effect of M on D")


```

```{r}
A_seq <- seq(from = -2, to = 2, length.out = 30)

post <- extract.samples(m5.3_A)
M_sim <- with(post, sapply(1:30,
  function(i) rnorm(1e3, aM + bAM*A_seq[i], sigma_M)))


D_sim <- with(post, sapply(1:30,
  function(i) rnorm(1e3, a + bA*A_seq[i] + bM*M_sim[,i],sigma)))

```


## Masked Relationship

Association between outcome and predictor masked by another variable.

Arises when there are two predictor variables that are correlated with one another. One is positive correlation and one is negative correlation.

The second reason to use more than one predictor variables is to measure direct influences of multiple factors on an outcome, when none of those influences is apparent from bivariate relationships.

milk dataset has NA's present. We need to use Complete Case Analysis

```{r}
data(milk)
d <- milk
str(d)

# Standardize 

d$K <- standardize(d$kcal.per.g)
d$N <- standardize(d$neocortex.perc)
d$M <- standardize(log(d$mass))

# m5.5_draft <- quap(
#   alist(
#     K ~ dnorm(mu, sigma),
#     mu <- a + bN*N,
#     a ~ dnorm(0,1),
#     bN ~ dnorm(0, 1),
#     sigma ~ dexp(1)
#   ), data = d
# )

d$neocortex.perc


# Use only complete cases
dcc <- d[complete.cases(d$K, d$N, d$M), ]

m5.5_draft <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bN*N,
    a ~ dnorm(0,1),
    bN ~ dnorm(0,1),
    sigma ~ dexp(1)
  ), data = dcc
)

```


Look at those prior

```{r}

prior <- extract.prior(m5.5_draft)
xseq <- c(-2,2)
mu <- link(m5.5_draft, post = prior, data = list(N = xseq))

plot(NULL, xlim = xseq, ylim = xseq)
for (i in 1:50) {
  lines(xseq, mu[i, ], col = col.alpha("black", 0.3))
}

```

Need to contrain prior distributions or this happens. We make them closer to 0.

```{r}

m5.5 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bN*N,
    a ~ dnorm(0,0.2),
    bN ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = dcc
)

precis(m5.5)

xseq <- seq(min(dcc$N)-0.15, max(dcc$N)+0.15, length.out = 30)
mu <- link(m5.5, data = list(N = xseq))
mu_mean <- apply(mu, 2, mean)
mu_PI <- apply(mu, 2, PI)

plot(K ~ N, data = dcc)
lines(xseq, mu_mean, lwd = 2)
shade(mu_PI, xseq)


```

Let's consider another predictor varialbe

```{r}

m5.6 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bM*M,
    a ~ dnorm(0,0.2),
    bM ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = dcc
)

precis(m5.6)

```

Combining the two models to make a multiple regression model

```{r}
m5.7 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a + bN*N + bM*M,
    a ~ dnorm(0,0.2),
    bN ~ dnorm(0, 0.5),
    bM ~ dnorm(0,0.5),
    sigma ~ dexp(1)
  ), data = dcc
)

precis(m5.7)

```

Let's plot them all together

```{r}

plot(coeftab(m5.5, m5.6, m5.7), pars = c("bM", "bN"))
```

The posterior means are moved away form zero when in one model. so adding both predictors seems to cause this


```{r}
xseq <- seq( from=min(dcc$M)-0.15 , to=max(dcc$M)+0.15 , length.out=30 )
mu <- link( m5.7 , data=data.frame( M=xseq , N=0 ) )
mu_mean <- apply(mu,2,mean)
mu_PI <- apply(mu,2,PI)
plot( NULL , xlim=range(dcc$M) , ylim=range(dcc$K) )
lines( xseq , mu_mean , lwd=2 )
shade( mu_PI , xseq )



```



### Simulating Masking Relationship

```{r}
# M -> K <- N
# M -> N
n <- 100
M <- rnorm( n )
N <- rnorm( n , M )
K <- rnorm( n , N - M )
d_sim <- data.frame(K=K,N=N,M=M)

# M -> K <- N
# N -> M
n <- 100
N <- rnorm( n )
M <- rnorm( n , N )
K <- rnorm( n , N - M )
d_sim2 <- data.frame(K=K,N=N,M=M)

# M -> K <- N
# M <- U -> N
n <- 100
U <- rnorm( n )
N <- rnorm( n , U )
M <- rnorm( n , U )
K <- rnorm( n , N - M )
d_sim3 <- data.frame(K=K,N=N,M=M)


dag5.7 <- dagitty("dag{ 
                  M -> K <- N
                  M -> N }")
coordinates(dag5.7) <- list( x = c(M = 0, K = 1, N = 2), y = c(M = 0, K = 1, N = 0.5))
MElist <- equivalentDAGs(dag5.7)


```

Creates list of all the DAGS

## Categorical Variables

Many predictors are discrete.

Two approaches

1. Use Dummy Variables

2. Use Index Variables

```{r index}

data(Howell1)
d <- Howell1

d$sex <- ifelse(d$male == 1, 2, 1)
str(d$sex)

```

This creates a list of alpha parameters one for each unique value in the index variable. we end up with a1 and a2

lets approximate the posterior

```{r}
m5.8 <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a[sex],
    a[sex] ~ dnorm(178, 20),
    sigma ~ dunif(0, 50)
  ), data = d
)

precis(m5.8, depth = 2)

# depth tells use to show any vector parameters

```

If we want to know the expected difference between these paramters?

sample from posterior

```{r}
post <- extract.samples(m5.8)
post$diff_fm <- post$a[,1] - post$a[,2]
precis(post, depth = 2)


```

When it gets more complicated you need to keep track of which index goes with which categories.

```{r}
data(milk)
d <- milk

# inspect levels
levels(d$clade)

# index the variable
d$clade_id <- as.integer(d$clade)

# random hogwarts assignment
set.seed(63)
d$house <- sample(rep(1:4, each = 8), size = nrow(d))

# Make model
m5.10 <- quap(
  alist(
    K ~ dnorm(mu, sigma),
    mu <- a[clade_id] + h[house],
    a[clade_id] ~ dnorm(0, 0.5),
    h[house] ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ), data = d
)

precis(m5.10, depth = 2)

labels <- paste("h[", 1:4, "]:", c("Gryffindor", "Hufflepuff", "Ravenclaw", "Slytherin"), sep = "")

plot(precis(m5.10, depth = 2, pars = "a"), labels = labels, xlab = "expected kcal (std)")

```


